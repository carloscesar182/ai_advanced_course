{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdNXs5oHj2RzxcbFrfWIHU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carloscesar182/ai_advanced_course/blob/main/Notebooks/NLP/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMrceYZ-L0xN"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "# palavras sem valor semantico\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# tecnicas que reduzem a palavra pra forma raiz\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer, LancasterStemmer\n",
        "\n",
        "# tecnicas de tokenização\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# tagging dizendo o que é cada palavra\n",
        "from nltk.tag import pos_tag, pos_tag_sents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download dos pacotes\n",
        "nltk.download('stopwords') # lista das stopwords\n",
        "nltk.download('punkt') # pontuações\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('tagsets_json') # pos tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng') # pos tagging\n",
        "nltk.download('maxent_ne_chunker_tab') # chunking\n",
        "nltk.download('words') # chunking\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "7QyXOIgCNGJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus é um conjunto de documentos\n",
        "texto = '''Nós somos feitos de poeira de estrelas. Nós somos uma maneira de o\n",
        "cosmos se autoconhecer. A imaginação nos leva a mundos que nunca sequer\n",
        "existiram. Mas sem ela não vamos a lugar algum.'''\n",
        "print(texto)"
      ],
      "metadata": {
        "id": "zneKEy5vOGBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenização de sentenças\n",
        "sentencas = sent_tokenize(texto)\n",
        "print(sentencas)"
      ],
      "metadata": {
        "id": "G1ayj07JOqFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ver a qty de sentenças\n",
        "print(len(sentencas))"
      ],
      "metadata": {
        "id": "VUGP2klyPMKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizar palavras\n",
        "token = word_tokenize(texto, language='portuguese')\n",
        "print(token)\n",
        "print(len(token))"
      ],
      "metadata": {
        "id": "k9MGFjXPPQWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords\n",
        "stops = stopwords.words('portuguese')\n",
        "print(stops)\n",
        "print(len(stops))"
      ],
      "metadata": {
        "id": "RyWOFdKbPyFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remover stopwords: pratica muito comum para algumas tecnicas de pln\n",
        "sem_stopwords = [palavra for palavra in token if palavra not in stops]\n",
        "print('Tokens: ', token)\n",
        "print('Sentença sem stopwords: ', sem_stopwords)\n",
        "print('Tamanho da sentença sem stopwords: ', len(sem_stopwords))"
      ],
      "metadata": {
        "id": "9XEJ2NaWQNGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remover pontuação\n",
        "print(string.punctuation)\n",
        "sem_pontuacao = [palavra for palavra in sem_stopwords if palavra not in string.punctuation]\n",
        "print('Sentença sem stopwords: ', sem_stopwords)\n",
        "print('Senteça sem stopwords e sem pontuação: ', sem_pontuacao)\n",
        "print('Tamanho da sentença sem stopwords e sem pontuação: ', len(sem_pontuacao))"
      ],
      "metadata": {
        "id": "bPpmpG9JQ9pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribuição de frequencia de cada token no corpus\n",
        "from nltk.probability import FreqDist\n",
        "freq = FreqDist(sem_pontuacao)\n",
        "freq"
      ],
      "metadata": {
        "id": "nCxhMBTcR4sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metodo most_common pra separar as palavras mais frequentes\n",
        "mais_comuns = freq.most_common(5)\n",
        "mais_comuns"
      ],
      "metadata": {
        "id": "gJJ_UUskSM9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemmers\n",
        "# porter\n",
        "stemmer = PorterStemmer()\n",
        "stem1 = [stemmer.stem(palavra) for palavra in sem_stopwords]\n",
        "print('Sem stopwords: ', sem_stopwords)\n",
        "print('Stem Porter: ', stem1)\n",
        "\n",
        "# snowball\n",
        "stemmer2 = SnowballStemmer('portuguese')\n",
        "stem2 = [stemmer2.stem(palavra) for palavra in sem_stopwords]\n",
        "print('Stem Snowball: ', stem2)\n",
        "\n",
        "# lancaster\n",
        "stemmer3 = LancasterStemmer()\n",
        "stem3 = [stemmer3.stem(palavra) for palavra in sem_stopwords]\n",
        "print('Stem Lancaster: ', stem3)"
      ],
      "metadata": {
        "id": "yETXovaNStse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos tagging: é o endereçamento do que é cada palavra (verbo, pronome, etc)\n",
        "\n",
        "# ver a documentação do pos tagging\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JUi6KaKjVV-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gerar um pos tagging das nossas palavras já processadas\n",
        "pos = nltk.pos_tag(sem_pontuacao)\n",
        "print(pos)"
      ],
      "metadata": {
        "id": "6QCV5KHiVul-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatizer: tecnica mais sofisticada que o stemming\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "resultado = [lemmatizer.lemmatize(palavra) for palavra in sem_stopwords]\n",
        "print('Sem stopwords: ', sem_stopwords)\n",
        "print('Lematização: ', resultado)"
      ],
      "metadata": {
        "id": "qMmYhBkYWYAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entidades nomeadas: reconhecer lugares, marcas, nomes no texto\n",
        "texto_en = \"Brasil é o país em que o Lula é o presidente\"\n",
        "token3 = word_tokenize(texto_en)\n",
        "tags = nltk.pos_tag(token3) # usamos pos tag pq ele faz a marcação que a palavra tem pra ajudar reconhecer a entidade nomeada\n",
        "en = nltk.ne_chunk(tags)\n",
        "print(en)"
      ],
      "metadata": {
        "id": "j0hGlNYvWvT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}